{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saneens/SpaceInvaderDoubleDQN/blob/main/DDQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HA6h8Xe_ZOzN",
        "outputId": "134ee5d7-9807-4de9-fdc9-54da35aae918",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcwO55KWBWE2",
        "outputId": "b9ea17ff-9bd6-43a9-b680-37ef3e788c27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "!pip install gym pyvirtualdisplay -qq\n",
        "!pip install folium==0.2.1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg -qq\n",
        "\n",
        "!apt-get update -qq\n",
        "!apt-get install cmake -qq\n",
        "!pip install --upgrade setuptools -qq\n",
        "!pip install ez_setup -qq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting folium==0.2.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/dd/75ced7437bfa7cb9a88b96ee0177953062803c3b4cde411a97d98c35adaf/folium-0.2.1.tar.gz (69kB)\n",
            "\r\u001b[K     |████▊                           | 10kB 21.6MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 20kB 26.1MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 30kB 30.2MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 40kB 33.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 51kB 36.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 61kB 38.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 71kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.6/dist-packages (from folium==0.2.1) (2.11.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2->folium==0.2.1) (1.1.1)\n",
            "Building wheels for collected packages: folium\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-cp36-none-any.whl size=79979 sha256=8eca8cefce5d509d4d9cb94427cee2b3c79b6510fb4389b365a758345f901f74\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/09/f0/52d2ef419c2aaf4fb149f92a33e0008bdce7ae816f0dd8f0c5\n",
            "Successfully built folium\n",
            "Installing collected packages: folium\n",
            "  Found existing installation: folium 0.8.3\n",
            "    Uninstalling folium-0.8.3:\n",
            "      Successfully uninstalled folium-0.8.3\n",
            "Successfully installed folium-0.2.1\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 144568 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.4_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.4) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "  Building wheel for ez-setup (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blNp43R_BhjN",
        "outputId": "060709a8-3ee3-4545-a06b-4858d4b943c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "!pip install blosc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting blosc\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/84/1ece24bd26f36d02f112941afdf0a7608bf001615c95a25034e500d54117/blosc-1.9.1.tar.gz (809kB)\n",
            "\r\u001b[K     |▍                               | 10kB 25.0MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 30.1MB/s eta 0:00:01\r\u001b[K     |█▏                              | 30kB 31.8MB/s eta 0:00:01\r\u001b[K     |█▋                              | 40kB 34.7MB/s eta 0:00:01\r\u001b[K     |██                              | 51kB 36.4MB/s eta 0:00:01\r\u001b[K     |██▍                             | 61kB 38.9MB/s eta 0:00:01\r\u001b[K     |██▉                             | 71kB 39.2MB/s eta 0:00:01\r\u001b[K     |███▎                            | 81kB 40.1MB/s eta 0:00:01\r\u001b[K     |███▋                            | 92kB 41.5MB/s eta 0:00:01\r\u001b[K     |████                            | 102kB 35.7MB/s eta 0:00:01\r\u001b[K     |████▌                           | 112kB 35.7MB/s eta 0:00:01\r\u001b[K     |████▉                           | 122kB 35.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 133kB 35.7MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 143kB 35.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 153kB 35.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 163kB 35.7MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 174kB 35.7MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 184kB 35.7MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 194kB 35.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 204kB 35.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 215kB 35.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 225kB 35.7MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 235kB 35.7MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 245kB 35.7MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 256kB 35.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 266kB 35.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 276kB 35.7MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 286kB 35.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 296kB 35.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 307kB 35.7MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 317kB 35.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 327kB 35.7MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 337kB 35.7MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 348kB 35.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 358kB 35.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 368kB 35.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 378kB 35.7MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 389kB 35.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 399kB 35.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 409kB 35.7MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 419kB 35.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 430kB 35.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 440kB 35.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 450kB 35.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 460kB 35.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 471kB 35.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 481kB 35.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 491kB 35.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 501kB 35.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 512kB 35.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 522kB 35.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 532kB 35.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 542kB 35.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 552kB 35.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 563kB 35.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 573kB 35.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 583kB 35.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 593kB 35.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 604kB 35.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 614kB 35.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 624kB 35.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 634kB 35.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 645kB 35.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 655kB 35.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 665kB 35.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 675kB 35.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 686kB 35.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 696kB 35.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 706kB 35.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 716kB 35.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 727kB 35.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 737kB 35.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 747kB 35.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 757kB 35.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 768kB 35.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 778kB 35.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 788kB 35.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 798kB 35.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 808kB 35.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 819kB 35.7MB/s \n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: blosc\n",
            "  Building wheel for blosc (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for blosc: filename=blosc-1.9.1-cp36-cp36m-linux_x86_64.whl size=458896 sha256=e14f1b81546cd96b66f608660835bfad375153e1052d9f0d402c2a72500c9617\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/c6/ad/3a62998b35309a61b84b306447c85bf639c5d539609c222c36\n",
            "Successfully built blosc\n",
            "Installing collected packages: blosc\n",
            "Successfully installed blosc-1.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0KQ41shBi-C",
        "outputId": "a3446b6e-89a8-4bd5-82a5-36d3af198274",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "!pip install --upgrade wandb -qq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 25.4MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 30.8MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 35.2MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 28.9MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 16.4MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61kB 15.1MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71kB 14.9MB/s eta 0:00:01\r\u001b[K     |██                              | 81kB 13.1MB/s eta 0:00:01\r\u001b[K     |██▏                             | 92kB 13.6MB/s eta 0:00:01\r\u001b[K     |██▍                             | 102kB 13.5MB/s eta 0:00:01\r\u001b[K     |██▋                             | 112kB 13.5MB/s eta 0:00:01\r\u001b[K     |██▉                             | 122kB 13.5MB/s eta 0:00:01\r\u001b[K     |███                             | 133kB 13.5MB/s eta 0:00:01\r\u001b[K     |███▎                            | 143kB 13.5MB/s eta 0:00:01\r\u001b[K     |███▌                            | 153kB 13.5MB/s eta 0:00:01\r\u001b[K     |███▉                            | 163kB 13.5MB/s eta 0:00:01\r\u001b[K     |████                            | 174kB 13.5MB/s eta 0:00:01\r\u001b[K     |████▎                           | 184kB 13.5MB/s eta 0:00:01\r\u001b[K     |████▌                           | 194kB 13.5MB/s eta 0:00:01\r\u001b[K     |████▊                           | 204kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████                           | 215kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 225kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 235kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 245kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 256kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 266kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 276kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 286kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 296kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 307kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 317kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 327kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 337kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████                        | 348kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 358kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 368kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 378kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 389kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 399kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 409kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 419kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 430kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 440kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 450kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 460kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████                     | 471kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 481kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 491kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 501kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 512kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████                    | 522kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 532kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 542kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 552kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 563kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 573kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 583kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 593kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 604kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 614kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 624kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 634kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 645kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 655kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 665kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 675kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 686kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 696kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 706kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 716kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 727kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 737kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 747kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 757kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 768kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 778kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 788kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 798kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 808kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 819kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 829kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 839kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 849kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 860kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 870kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 880kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 890kB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 901kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 911kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 921kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 931kB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 942kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 952kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 962kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 972kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 983kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 993kB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.0MB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.0MB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.0MB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.0MB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.0MB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.1MB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.1MB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.1MB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.1MB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.1MB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.1MB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.1MB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.1MB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.1MB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.1MB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.2MB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2MB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.2MB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.2MB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.2MB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.2MB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.2MB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.2MB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.2MB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.2MB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.3MB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.3MB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.3MB 13.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.3MB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.3MB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.3MB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.3MB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.3MB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.3MB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.4MB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.4MB 13.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.4MB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4MB 13.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4MB 13.5MB/s \n",
            "\u001b[?25h\u001b[?25l\r\u001b[K     |███▏                            | 10kB 29.8MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 20kB 35.3MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 30kB 41.1MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 40kB 44.2MB/s eta 0:00:01\r\u001b[K     |████████████████                | 51kB 46.3MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 61kB 49.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 71kB 49.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 81kB 49.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 92kB 50.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 102kB 52.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 52.0MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 15.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 102kB 14.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 460kB 45.8MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 71kB 11.2MB/s \n",
            "\u001b[?25h  Building wheel for gql (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for watchdog (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for graphql-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-t2ChM2LB781",
        "outputId": "a4a2e9df-396e-4d94-e391-be6b3b155898",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(30)\n",
        "\n",
        "import csv\n",
        "import scipy.ndimage as ndimage\n",
        "import bisect\n",
        "import blosc\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import os\n",
        "import cv2\n",
        "import base64\n",
        "import tensorflow as tf\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from collections import deque\n",
        "from datetime import datetime\n",
        "import keras\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from keras import losses\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Flatten, Dense\n",
        "from keras.models import load_model\n",
        "\n",
        "from IPython.display import HTML\n",
        "from IPython import display as ipythondisplay\n",
        "from pyvirtualdisplay import Display\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "# import wandb\n",
        "import wandb\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiLXX9E1B9w-"
      },
      "source": [
        "#Class to manage the environment, makes it easier to control and perform actions on the environment.\n",
        "#Invokes function to preprocess the image and stack four frames together\n",
        "#Also takes care of the gym environment not displaying all the details in a frame\n",
        "#Nomenclature\n",
        "#step_number : Total steps performed by the environment, 4 actions are counted as a step. Same action is repeated four times everytime\n",
        "#episode_step : Steps performed by the env. in the current game\n",
        "#game_number : Number of game the environment is playing currently\n",
        "class GameAgent:\n",
        "    def __init__(self, args):\n",
        "        self.env = gym.make(\"SpaceInvaders-v0\")\n",
        "        self.action_size = self.env.action_space.n\n",
        "        self.game_number = 0\n",
        "        self.step_number = 0\n",
        "        self.epoch_number = 0\n",
        "        self.game_done = True\n",
        "        self.reset_game()\n",
        "\n",
        "    def reset_game(self):\n",
        "        if self.game_done:\n",
        "            self.game_number += 1\n",
        "        self.current_frame = self.env.reset()\n",
        "        self.game_done = False\n",
        "        self.lives_left = 3\n",
        "        self.state = State().state_plus_frame(self.current_frame)\n",
        "        self.game_score = 0\n",
        "        self.episode_step = 0\n",
        "\n",
        "    def take_action(self, action):\n",
        "        reward = 0\n",
        "        terminal = 0\n",
        "        self.step_number += 1\n",
        "        self.episode_step += 1\n",
        "\n",
        "        for i in range(4):\n",
        "            self.previous_frame = self.current_frame\n",
        "            self.current_frame, tmp_rwd, self.game_done, life = self.env.step(action)\n",
        "            reward += tmp_rwd\n",
        "\n",
        "            # Detect end of episode by comparing the life remaining of the agent\n",
        "            if life.get('ale.lives') < self.lives_left or self.game_done:\n",
        "                terminal = 1\n",
        "                self.lives_left = life.get('ale.lives')\n",
        "                break\n",
        "\n",
        "        #take the max of adjacent frames to avoid the gym rendering problem\n",
        "        maxed_frame = np.maximum(self.previous_frame, self.current_frame)\n",
        "        self.state = self.state.state_plus_frame(maxed_frame)\n",
        "        self.game_score += reward\n",
        "        return reward, self.state, terminal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKYVaZ31CYJs"
      },
      "source": [
        "#Class to do the preprocessing and stacking of four images together\n",
        "class State:\n",
        "    def preprocess(self,frame):\n",
        "        frame = frame[:, :, 0] * 0.2126 + frame[:, :, 1] * 0.0722 + frame[:, :, 2] * 0.7152\n",
        "        frame = frame.astype(np.uint8)\n",
        "        frame = cv2.resize(frame, (84, 84), interpolation=cv2.INTER_LINEAR)\n",
        "        frame.resize((84, 84, 1))\n",
        "        return frame\n",
        "\n",
        "    def state_plus_frame(self, frame):\n",
        "        frame = self.preprocess(frame)\n",
        "        frame = blosc.compress(np.reshape(frame, 84 * 84).tobytes(), typesize=1)\n",
        "        new_state = State()\n",
        "        if hasattr(self, 'screens'):\n",
        "            new_state.screens = self.screens[:3]\n",
        "            new_state.screens.insert(0, frame)\n",
        "        else:\n",
        "            new_state.screens = [frame, frame, frame, frame]\n",
        "        return new_state\n",
        "\n",
        "    def get_screens(self):\n",
        "        s = []\n",
        "        for i in range(4):\n",
        "            s.append(np.reshape(np.fromstring(blosc.decompress(self.screens[i]), dtype=np.uint8), (84, 84, 1)))\n",
        "        s = np.concatenate(s, axis=2)\n",
        "        s = np.reshape(s, (1, 84, 84, 4))\n",
        "        return s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnZdpmMmCZqa"
      },
      "source": [
        "class MemoryType:\n",
        "    def __init__(self, current_state, action, reward, next_state, terminal):\n",
        "        self.current_state = current_state\n",
        "        self.action = action\n",
        "        self.reward = reward\n",
        "        self.next_state = next_state\n",
        "        self.terminal = terminal\n",
        "        self.weight = 1\n",
        "        self.cumulative_weight = 1\n",
        "\n",
        "    def isInteresting(self):\n",
        "        return self.terminal or self.reward != 0\n",
        "\n",
        "    def __lt__(self, obj):\n",
        "        return self.cumulative_weight < obj.cumulative_weight\n",
        "\n",
        "\n",
        "class StorageMemory:\n",
        "\n",
        "    def __init__(self, args):\n",
        "        self.memory = []\n",
        "        self.max_capacity = args.replay_capacity\n",
        "        self.prioritized_replay = args.prioritized_replay\n",
        "        self.num_interesting_memory = 0\n",
        "        self.batches_drawn = 0\n",
        "\n",
        "    def add_memory(self, sample):\n",
        "        self.memory.append(sample)\n",
        "        self.update_weights()\n",
        "        self.truncate_memory()\n",
        "\n",
        "    def update_weights(self):\n",
        "        if len(self.memory) > 1:\n",
        "            self.memory[-1].cumulative_weight = self.memory[-1].weight + self.memory[-2].cumulative_weight\n",
        "\n",
        "        if self.memory[-1].isInteresting():\n",
        "            self.num_interesting_memory += 1\n",
        "\n",
        "            # Boost the neighboring memory.  How many memory?  Roughly the number of memory\n",
        "            uninteresting_range = int(max(1, len(self.memory) / max(1, self.num_interesting_memory)))\n",
        "            for i in range(uninteresting_range, 0, -1):\n",
        "                index = len(self.memory) - i\n",
        "                if index < 1:\n",
        "                    break\n",
        "                # This is an exponential that ranges from 3.0 to 1.01 over the domain of [0, uninteresting_range]\n",
        "                # So the interesting sample gets a 3x boost, and the one furthest away gets a 1% boost\n",
        "                boost = 1.0 + 3.0/(math.exp(i/(uninteresting_range/6.0)))\n",
        "                self.memory[index].weight *= boost\n",
        "                self.memory[index].cumulative_weight = self.memory[index].weight + self.memory[index - 1].cumulative_weight\n",
        "\n",
        "    def truncate_memory(self):\n",
        "        if len(self.memory) > self.max_capacity * 1.05:\n",
        "            truncated_weight = 0\n",
        "            # Before truncating the list, correct self.num_interesting_memory, and prepare\n",
        "            # for correcting the cumulativeWeights of the remaining memory\n",
        "            for i in range(self.max_capacity, len(self.memory)):\n",
        "                truncated_weight += self.memory[i].weight\n",
        "                if self.memory[i].isInteresting():\n",
        "                    self.num_interesting_memory -= 1\n",
        "\n",
        "            # Truncate the list\n",
        "            self.memory = self.memory[(len(self.memory) - self.max_capacity):]\n",
        "\n",
        "            # Correct cumulativeWeights\n",
        "            for sample in self.memory:\n",
        "                sample.cumulative_weight -= truncated_weight\n",
        "\n",
        "    def draw_batch(self, batch_size):\n",
        "        if batch_size > len(self.memory):\n",
        "            raise IndexError('Too few memory (%d) to draw a batch of %d' % (len(self.memory), batch_size))\n",
        "\n",
        "        self.batches_drawn += 1\n",
        "\n",
        "        if self.prioritized_replay:\n",
        "            return self.draw_prioritized_batch(batch_size)\n",
        "        else:\n",
        "            return random.sample(self.memory, batch_size)\n",
        "\n",
        "    # The nature paper doesn't do this but they mention the idea. see the results without this and remove\n",
        "    def draw_prioritized_batch(self, batch_size):\n",
        "        batch = []\n",
        "        probe = MemoryType(None, 0, 0, None, False)\n",
        "        while len(batch) < batch_size:\n",
        "            probe.cumulative_weight = random.uniform(0, self.memory[-1].cumulative_weight)\n",
        "            index = bisect.bisect_right(self.memory, probe, 0, len(self.memory) - 1)\n",
        "            sample = self.memory[index]\n",
        "            sample.weight = max(1, .8 * sample.weight)\n",
        "            if sample not in batch:\n",
        "                batch.append(sample)\n",
        "\n",
        "        if self.batches_drawn % 100 == 0:\n",
        "            cumulative = 0\n",
        "            for sample in self.memory:\n",
        "                cumulative += sample.weight\n",
        "                sample.cumulative_weight = cumulative\n",
        "        return batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZV66mHwCa52"
      },
      "source": [
        "def huber_loss_custom(y_actual, y_predicted):\n",
        "    difference = tf.keras.backend.abs(y_actual - y_predicted)\n",
        "    quadratic_part = tf.keras.backend.clip(difference, 0.0, 1.0)\n",
        "    linear_part = difference - quadratic_part\n",
        "    errors = (0.5 * tf.keras.backend.square(quadratic_part)) + linear_part\n",
        "    loss = tf.keras.backend.sum(errors)\n",
        "    return loss\n",
        "\n",
        "\n",
        "class SingleBiasLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, units, *args, **kwargs):\n",
        "        super(SingleBiasLayer, self).__init__(*args, **kwargs)\n",
        "        self.units = units\n",
        "\n",
        "    def build(self, units):\n",
        "        self.bias = self.add_weight('bias',\n",
        "                                    shape=[self.units],\n",
        "                                    initializer='zeros',\n",
        "                                    trainable=True)\n",
        "        super(SingleBiasLayer, self).build(units)  # Be sure to call this somewhere!\n",
        "\n",
        "    def call(self, x):\n",
        "        return tf.add(x ,self.bias)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {'units': self.units}\n",
        "        base_config = super(SingleBiasLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSHhVejyCbo-"
      },
      "source": [
        "class DQN:\n",
        "    def __init__(self, action_size, out_dir, args):\n",
        "\n",
        "        self.out_dir = out_dir\n",
        "        self.target_model_update_freq = args.target_model_update_freq\n",
        "        self.gamma = args.gamma\n",
        "        self.learning_rate = args.learning_rate\n",
        "\n",
        "        self.epoch_total_qval = 0\n",
        "        self.epoch_train_step = 0\n",
        "        self.epoch_loss = 0\n",
        "\n",
        "\n",
        "        print(\"Building network for Policy Model\")\n",
        "        self.policy_model = self.buildNetwork()\n",
        "        print(\"Building network for Target Model\")\n",
        "        self.target_model = self.buildNetwork()\n",
        "\n",
        "        self.target_model.set_weights(self.policy_model.get_weights())\n",
        "\n",
        "    def buildNetwork(self):\n",
        "\n",
        "        frames_input = tf.keras.Input(shape=(84,84,4), name='frames')\n",
        "        actions_input = tf.keras.Input(shape=(6,), name='filter')\n",
        "        conv_1 = layers.Conv2D(32, (8, 8), strides=(4,4), input_shape= (84,84,4),\n",
        "                                activation = 'relu', padding= 'valid',\n",
        "                                data_format='channels_last', use_bias=True)(tf.keras.layers.Lambda(lambda x: x /255.0)(frames_input))\n",
        "        conv_2 = layers.Conv2D(64, (4, 4), strides=(2,2), input_shape= (84,84,4),\n",
        "                                activation = 'relu', padding= 'valid',\n",
        "                                data_format='channels_last', use_bias=True)(conv_1)\n",
        "        conv_3 = layers.Conv2D(64, (3, 3), strides=(1,1), input_shape= (84,84,4),\n",
        "                                activation = 'relu', padding= 'valid',\n",
        "                                data_format='channels_last', use_bias=True)(conv_2)\n",
        "        conv_flattened = layers.Flatten()(conv_3)\n",
        "        hidden = layers.Dense(512, activation ='relu', use_bias=True)(conv_flattened)\n",
        "        output_wo_bias = layers.Dense(6, activation='linear', use_bias=False)(hidden)\n",
        "        output = SingleBiasLayer(units=1)(output_wo_bias)\n",
        "        filtered_output = layers.multiply([output, actions_input])\n",
        "        model = models.Model(inputs = [frames_input, actions_input], outputs = filtered_output)\n",
        "        model.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=self.learning_rate,rho=0.95,epsilon=0.01), loss='mse', metrics=[\"accuracy\"])\n",
        "        model.summary()\n",
        "        return model\n",
        "\n",
        "    def predict(self, screens, action_mask = None):\n",
        "        if action_mask is None:\n",
        "            action_mask = np.array([1,1,1,1,1,1]).reshape(1,6)\n",
        "        q_values = self.policy_model.predict([screens,action_mask], batch_size = 1)\n",
        "        q_values = np.squeeze(q_values)\n",
        "        return np.argmax(q_values)\n",
        "\n",
        "    def train(self, minibatch, step_number):\n",
        "\n",
        "        next_screens_arr = [obj.next_state.get_screens()[0] for obj in minibatch]\n",
        "        action_mask_target = np.ones((len(minibatch), 6))\n",
        "        next_state_qval_arr_t = self.target_model.predict_on_batch([np.array(next_screens_arr),action_mask_target])\n",
        "        next_state_qval_arr_p = self.policy_model.predict_on_batch([np.array(next_screens_arr),action_mask_target])\n",
        "\n",
        "        current_screens_arr = [obj.current_state.get_screens()[0] for obj in minibatch]\n",
        "\n",
        "        action_mask_policy = np.zeros((len(minibatch), 6))\n",
        "        current_state_qval_arr = np.zeros((len(minibatch),6))\n",
        "\n",
        "        for i in range(0, len(minibatch)):\n",
        "            action_mask_policy[i][minibatch[i].action] = 1\n",
        "            if minibatch[i].terminal:\n",
        "                current_state_qval_arr[i][minibatch[i].action] = minibatch[i].reward\n",
        "            else:\n",
        "                current_state_qval_arr[i][minibatch[i].action] = minibatch[i].reward + self.gamma * next_state_qval_arr_t[i][np.argmax(next_state_qval_arr_p[i])]\n",
        "\n",
        "        self.hist = self.policy_model.fit([np.asarray(current_screens_arr),action_mask_policy], current_state_qval_arr, batch_size = len(minibatch), verbose=0)\n",
        "\n",
        "        self.epoch_total_qval += np.sum(current_state_qval_arr)\n",
        "        self.epoch_train_step += 1\n",
        "        self.epoch_loss += self.hist.history['loss'][0]\n",
        "\n",
        "        if step_number % self.target_model_update_freq == 0:\n",
        "            self.target_model.set_weights(self.policy_model.get_weights())\n",
        "\n",
        "    def save_model(self,name=\"model.h5\"):\n",
        "        self.policy_model.save(self.out_dir + \"/\" +name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5szjVGZlCdxe"
      },
      "source": [
        "class GameTrainer:\n",
        "    def __init__(self,out_dir):\n",
        "        self.out_dir = out_dir\n",
        "        self.args = Parameters()\n",
        "        self.epoch_steps = self.args.epoch_steps\n",
        "        self.agent = GameAgent(self.args)\n",
        "        self.dqn = DQN(self.agent.action_size, self.out_dir, self.args)\n",
        "        self.storage = StorageMemory(self.args)\n",
        "\n",
        "        self.eval_epoch_num = 0\n",
        "        self.train_epoch_num = 0\n",
        "        self.train_game_count = 0\n",
        "        self.stats = []\n",
        "\n",
        "\n",
        "    def update_epsilon(self):\n",
        "        self.epsilon = max(0.01, (1.0 - 0.9 * self.agent.step_number / 1000000))\n",
        "\n",
        "    def train_epoch(self, num_epochs, eval_num_games = 100):\n",
        "        for i in range(num_epochs):\n",
        "            self.train_epoch_num += 1\n",
        "\n",
        "            step_start = self.agent.step_number\n",
        "            epoch_score_total = 0\n",
        "            epoch_game_count = 0\n",
        "\n",
        "            while((self.agent.step_number - step_start) < self.epoch_steps ):\n",
        "                state = None\n",
        "                game_score = 0\n",
        "                no_op_action = 0\n",
        "                no_op_max = random.randint(0,30)\n",
        "\n",
        "                while (not self.agent.game_done):\n",
        "                    self.update_epsilon()\n",
        "\n",
        "                    if state is None or random.random() > (1 - self.epsilon):\n",
        "                        action =  random.randrange(self.agent.action_size)\n",
        "                    else:\n",
        "                        screens = state.get_screens()\n",
        "                        action = self.dqn.predict(screens)\n",
        "\n",
        "                    if no_op_action < no_op_max:\n",
        "                        action = 0\n",
        "                        no_op_action += 1\n",
        "\n",
        "                    old_state = state\n",
        "                    reward, state, terminal = self.agent.take_action(action)\n",
        "\n",
        "                    game_score += reward\n",
        "\n",
        "                    if old_state is not None:\n",
        "                        clipped_reward = min(1, max(-1,reward))\n",
        "                        # clipped_reward = reward\n",
        "                        self.storage.add_memory(MemoryType(old_state, action, clipped_reward, state, terminal))\n",
        "\n",
        "                        if (self.agent.step_number > self.args.observation_steps) and (self.agent.episode_step % 4 == 0): #maybe delete %4\n",
        "                            minibatch = self.storage.draw_batch(64) #WANDB\n",
        "                            self.dqn.train(minibatch, self.agent.step_number)\n",
        "\n",
        "                    if terminal:\n",
        "                        state = None\n",
        "                self.agent.reset_game()\n",
        "                epoch_score_total += game_score\n",
        "                epoch_game_count += 1\n",
        "                self.train_game_count += 1\n",
        "\n",
        "                self.stats.append([self.agent.step_number, game_score])\n",
        "\n",
        "\n",
        "            with open(self.out_dir + '/training.csv', mode='a') as self.training_csv:\n",
        "                self.training_writer = csv.writer(self.training_csv, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "                for log_ind in range(len(self.stats)):\n",
        "                    self.training_writer.writerow(self.stats[log_ind])\n",
        "                self.training_csv.close()\n",
        "\n",
        "            with open(self.out_dir + '/training_epoch.csv', mode='a') as self.training_epoch_csv:\n",
        "                self.training_epoch_writer = csv.writer(self.training_epoch_csv, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "                self.training_epoch_writer.writerow([self.train_epoch_num, epoch_game_count, epoch_score_total/epoch_game_count])\n",
        "                self.training_epoch_csv.close()\n",
        "            #WANDB\n",
        "            #WANDB\n",
        "            wandb.log({'epoch_score': epoch_score_total/epoch_game_count})\n",
        "            wandb.log({'epoch_game_count': epoch_game_count})\n",
        "            wandb.log({'epoch_loss_total': self.dqn.epoch_loss})\n",
        "            wandb.log({'epoch_avg_qval': self.dqn.epoch_total_qval/self.dqn.epoch_train_step})\n",
        "            self.dqn.epoch_total_qval = 0\n",
        "            self.dqn.epoch_train_step = 0\n",
        "            self.dqn.epoch_loss = 0\n",
        "\n",
        "\n",
        "            self.stats = []\n",
        "            self.agent.epoch_number += 1\n",
        "            # self.eval_epoch(eval_num_games)\n",
        "\n",
        "    def eval_epoch(self,num_games):\n",
        "        play_agent = GameAgent(self.args)\n",
        "        self.eval_epoch_num += 1\n",
        "        total_score = 0\n",
        "        for game in range(num_games):\n",
        "            play_agent.reset_game()\n",
        "            done = False\n",
        "            life = 0\n",
        "            game_score = 0\n",
        "            state = None\n",
        "            while not done:\n",
        "                if(random.random() > 0 and state is not None):\n",
        "                    screens = state.get_screens()\n",
        "                    action = self.dqn.predict(screens)\n",
        "                else:\n",
        "                    action = random.randrange(play_agent.action_size)\n",
        "\n",
        "                reward, state, terminal = play_agent.take_action(action)\n",
        "\n",
        "                life += terminal\n",
        "                game_score += reward\n",
        "                if(life == 3):\n",
        "                    done = True\n",
        "            total_score += game_score\n",
        "\n",
        "        average_score = total_score / num_games\n",
        "        with open(self.out_dir + '/evaluation_epoch.csv', mode='a') as self.evaluation_epoch_csv:\n",
        "            self.evaluation_epoch_writer = csv.writer(self.evaluation_epoch_csv, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
        "            self.evaluation_epoch_writer.writerow([self.eval_epoch_num, average_score])\n",
        "            self.evaluation_epoch_csv.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bv7ZQ5JzCf5E"
      },
      "source": [
        "class Parameters:\n",
        "    def __init__(self):\n",
        "        self.replay_capacity = 1000000 #state to store for training\n",
        "        self.target_model_update_freq =  10000 #steps to update target model\n",
        "        self.learning_rate = 0.00025 #learning rate\n",
        "        self.gamma = 0.99 #gamma for Q learning\n",
        "        self.observation_steps = 70 #training starts after observation_steps\n",
        "        self.prioritized_replay = False #Prioritize critical states when training (e.g. terminal or non zero rewards)\")\n",
        "        self.epoch_steps = 100000 #Number of steps to perform in an epoch\n",
        "\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F122L5pdChC2",
        "outputId": "73d9785f-e4bd-4d1e-eaf0-4af3647b0301",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd '/content/drive/My Drive/WANDB'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/WANDB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R733FZu7CvLK",
        "outputId": "0cb565cd-c079-413a-8c48-6f5fb451f363",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "!wandb login xxxxxxxxxxxxxxxxxxxx\n",
        "id = \"config_7\"\n",
        "wandb.init(project=\"space-invaders-dueling-network\", id=id, resume=\"allow\")\n",
        "\n",
        "wandb.config.batch_size = 64\n",
        "wandb.config.learning_rate = 0.00025\n",
        "wandb.config.optimizer = 'RMS'\n",
        "wandb.config.taget_update_freq = 10000\n",
        "wandb.config.loss = 'MSE'\n",
        "\n",
        "\n",
        "# agent.save(os.path.join(wandb.run.dir, \"model.h5\"))\n",
        "# wandb.log({'cumulative_avg_reward': cumulative_avg_reward})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[32mSuccessfully logged in to Weights & Biases!\u001b[0m\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Logging results to <a href=\"https://wandb.com\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://app.wandb.ai/saneens/space-invaders-dueling-network\" target=\"_blank\">https://app.wandb.ai/saneens/space-invaders-dueling-network</a><br/>\n",
              "                Run page: <a href=\"https://app.wandb.ai/saneens/space-invaders-dueling-network/runs/config_7\" target=\"_blank\">https://app.wandb.ai/saneens/space-invaders-dueling-network/runs/config_7</a><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Streaming file created twice in same run: /content/drive/My Drive/WANDB/wandb/run-20200429_233024-config_7/wandb-events.jsonl\n",
            "Streaming file created twice in same run: /content/drive/My Drive/WANDB/wandb/run-20200429_233024-config_7/wandb-history.jsonl\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7D6vKptHCjQS"
      },
      "source": [
        "baseOutputDir = 'config_7'\n",
        "if not os.path.isdir(baseOutputDir):\n",
        "    os.makedirs(baseOutputDir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDeQvoXqCkzx",
        "outputId": "832a16ee-abab-4c1c-c111-38196aa1004d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Create the trainer\n",
        "trainer = GameTrainer(baseOutputDir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building network for Policy Model\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "frames (InputLayer)             [(None, 84, 84, 4)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 84, 84, 4)    0           frames[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 20, 20, 32)   8224        lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 9, 9, 64)     32832       conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 7, 7, 64)     36928       conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 3136)         0           conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          1606144     flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 6)            3072        dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "single_bias_layer (SingleBiasLa (None, 6)            1           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "filter (InputLayer)             [(None, 6)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "multiply (Multiply)             (None, 6)            0           single_bias_layer[0][0]          \n",
            "                                                                 filter[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,687,201\n",
            "Trainable params: 1,687,201\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Building network for Target Model\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "frames (InputLayer)             [(None, 84, 84, 4)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 84, 84, 4)    0           frames[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 20, 20, 32)   8224        lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 9, 9, 64)     32832       conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 7, 7, 64)     36928       conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 3136)         0           conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 512)          1606144     flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 6)            3072        dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "single_bias_layer_1 (SingleBias (None, 6)            1           dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "filter (InputLayer)             [(None, 6)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "multiply_1 (Multiply)           (None, 6)            0           single_bias_layer_1[0][0]        \n",
            "                                                                 filter[0][0]                     \n",
            "==================================================================================================\n",
            "Total params: 1,687,201\n",
            "Trainable params: 1,687,201\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u98UKgoCm-w",
        "outputId": "8f8dadc1-7761-4326-b571-7f326766fa0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if(os.path.exists(baseOutputDir + '/GameAgent_obj')):\n",
        "    with open(baseOutputDir + '/GameAgent_obj', 'rb') as input:\n",
        "        trainer.agent = pickle.load(input)\n",
        "    with open(baseOutputDir + '/Storage_obj', 'rb') as input:\n",
        "        trainer.storage = pickle.load(input)\n",
        "    trainer.dqn.policy_model.load_weights(baseOutputDir + '/model_' + str(trainer.agent.epoch_number) + '.h5')\n",
        "    trainer.dqn.target_model.set_weights(trainer.dqn.policy_model.get_weights())\n",
        "    # with open(baseOutputDir +'/optimizer.pkl', 'rb') as f:\n",
        "    #     weight_values = pickle.load(f)\n",
        "    #     trainer.dqn.policy_model.optimizer.set_weights(weight_values)\n",
        "    print(\"Done loading prev checkpoint\")\n",
        "else:\n",
        "    print('No files found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Done loading prev checkpoint\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmdhF172CnTS",
        "outputId": "fa62a972-1ca4-4391-ccba-c9d639b18f47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Start the training, specify the number of epochs\n",
        "while True:\n",
        "    trainer.train_epoch(1, 100)\n",
        "    trainer.dqn.save_model('model_'+ str(trainer.agent.epoch_number) + '.h5')\n",
        "    trainer.dqn.save_model('model.h5')\n",
        "    with open(baseOutputDir + '/GameAgent_obj', 'wb') as output:\n",
        "        pickle.dump(trainer.agent, output, pickle.HIGHEST_PROTOCOL)\n",
        "    with open(baseOutputDir + '/Storage_obj', 'wb') as output:\n",
        "        pickle.dump(trainer.storage, output, pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    # symbolic_weights = getattr(trainer.dqn.policy_model.optimizer, 'weights')\n",
        "    # weight_values = tf.keras.backend.batch_get_value(symbolic_weights)\n",
        "    # with open(baseOutputDir +'/optimizer.pkl', 'wb') as f:\n",
        "    #     pickle.dump(weight_values, f)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XPNn2OGhqnZ"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}